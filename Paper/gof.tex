\RequirePackage[]{amsmath}
\documentclass[]{svjour3}


\usepackage{mathrsfs}
\usepackage[caption=false]{subfig}% Support for small, `sub' figures and tables
\usepackage[numbers,sort&compress]{natbib}% Citation support using natbib.sty
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{float}
\usepackage{lmodern}
\setcitestyle{authoryear,open={(},close={)}}
\newcommand{\pkg}[1]{{\normalfont\fontseries{b}\selectfont #1}} \let\proglang=\textsf \let\code=\texttt

\begin{document}

\title{A Comparison of Goodness-of-Fit Tests for Univariate Data}
\author{Dr. Wolfgang Rolke}
\journalname{Computational Statistics}
\date{9/24/2023}
\institute{Dept. of Mathematical Sciences, University of Puerto Rico - Mayaguez
\email{wolfgang.rolke@upr.edu}}
\maketitle

\begin{abstract}

Testing to see whether a given data set comes from some specified distribution is among the oldest types of problems in Statistics. Many such tests have been developed and their performance studied. The general result has been that while a certain test might perform well, aka have good power, in one situation it will fail badly in others. This is not a surprise given the great many ways in which a distribution can differ from the one specified in the null hypothesis. It is therefore very difficult to decide a priori which test to use. The obvious solution is not to rely on any one test but to run several of them. This however leads to the problem of simultaneous inference, that is, if several tests are done even if the null hypothesis were true, one of them is likely to reject it anyway just by random chance. In this paper we present the results of extensive simulation studies for both continuous and discrete data. Based on these studies we make recommendations for a fairly small number of methods chosen in such a way that at least one of them has good power in each of the included scenarios. 

\keywords{Anderson-Darling, Kolmogorov-Smirnov, Monte Carlo Simulation, Continuous, Discrete} 
\end{abstract}


\section{Introduction}


A goodness-of-fit (gof) test is concerned with the question whether a data set has been generated by a certain distribution. It has a null
hypothesis of the form \(H_0: F=F_0\), where \(F_0\) is a probability
distribution. For example, one might wish to test whether a data set
comes from a standard normal distribution. An obvious and usually more
useful extension is to test \(H_0: F \in \mathscr{F}_0\) where \(\mathscr{F}_0\) is a family of
distributions but without specifying the parameters. So one might wish
to test whether a data set comes from a normal distribution but without
specifying the mean and standard deviation. 

As described above a goodness-of-fit test is a hypothesis test in the
Fisherian sense of testing whether the data is in agreement with a
model. The main issue with this approach is that it does not allow one to
decide which of two tests is better, that is has the higher power. To solve this problem
Neyman and Pearson in the 1930s introduced the concept of an alternative
hypothesis, and most tests done today follow more closely the
Neyman-Pearson description, although they often are a hybrid of both.
The original Fisherian test survives mostly in the goodness-of-fit
problem, because here the obvious alternative is \(H_a: F \not\in \mathscr{F}_0\), a
space so huge as to be useless for power calculations. 

Most studies of goodness-of-fit tests focus on continuous data. Of course discrete data can be of interest in its own right, but there are also two additional situations where one might have to use methods for discrete data. First one encounters the situation where the data is actually continuous but the data set is so large that running a simulation based goodness-of-fit test becomes computationally expensive. One option then is to discretize the data and run a discrete test. Another case is where data while in principle continuous is collected in such a way that it becomes discrete. One example for this is data in high energy physics and astronomy, because of finite resolution of the measurement apparatus.

It is also important to note that while many methods exist for both continuous and discrete data and even carry the same name, there are often differences in the formulas for calculating the test statistics. Moreover, a method might perform well for continuous data but not for discrete  data, and vice versa.

Many of the tests included in this study have known null distributions for finding p-values. However, these generally require a sufficiently large sample size. Moreover, they generally do not allow for parameter estimation and fail for discrete data. Finally, some of the tests require simulation for finding p-values. For these reasons in our study simulation is used exclusively for the calculation of the p-values and the power of the tests, with the exception of the chi-square tests..

The goodness-of-fit test is one of the oldest and most studied problems in
Statistics. For an introduction to Statistics and hypothesis testing in
general see \citet{casella2002} or \citet{bickel2015}. For general discussions of the many
goodness-of-fit tests available see \citet{agostini1986}, \citet{raynor2009}, \citet{zhang2002} and  \citet{thas2010}. \citet{thas2010} has an extensive list of references on the subject. 

\section{Software}

The study was done with \proglang{R} \citet{r2021} on \emph{Rstudio}
\citet{rstudio2020} and the package \emph{Rgof} (Rolke 2023), available from CRAN.  \emph{Rgof} includes routines to carry out the tests and to run power studies. These routines also allow the user to use methods not already implemented. 

\section{The Methods}\label{the-methods}
In the following we list the methods included in the study. Most
are well known and have been in use for a long time. For their details
see the references.

The methods included in this study are all true omnibus methods, that is not designed to work especially well for any one specific alternative. Moreover, they are methods that do not require the choice of a tuning parameter. So for example we did not include any variant of Neyman's smooth method \citet{neyman1937} because it only works well when tuned to a specific alternative.

\subsection{Continuous Data/Model}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Kolmogorov Smirnov (KS) \citet{massey1951}, \citet{kolmogorov1933}, \citet{smirnov1948}, citet{birnbaum1952}. Maybe the most famous of all goodness-of-fit tests, it is based
  on the largest absolute difference between the empirical and the
  theoretical distribution function.
\item
  Kuiper (K) \citet{kuiper1960} A variation of the KS test, it is based on
  the sum of the largest positive and negative differences between the
  empirical and the theoretical distribution function.
\item
  Anderson-Darling (AD) \citet{anderson1952}, \citet{anderson1954} This tests finds the integrated mean square of the
  difference between the empirical and the theoretical distribution
  function, with the left end right endpoints inflated.
\item
  Cramer-vonMises (CvM) \citet{anderson1962} This tests finds the integrated
  mean square of the difference between the empirical and the
  theoretical distribution function.
\item
  Wilson (W) A variation of the CvM test, it adjusts the CvM test by
  subtracting the difference of the mean empirical distribution function
  from \(\frac12\).
\item
  Zhang's methods (ZA, ZK, ZC) \citet{zhang2002} Three tests based on the
  likelihood ratio.
\item
  Wasserstein p=1 (Wassp1) \citet{wasserstein1999} A test based on the
  Wasserstein distance, well known distance measure for probability
  distributions.

For all of these tests the distribution of the test statistic under the
null hypothesis is found via simulation.

\item
  Eight variations of chi square tests, using the formula by Pearson \citet{pearson1900} or
  based on the likelihood ratio test, bins of equal size or equal
  probability and both a large number and a small number of bins, 100
  and 10 by default. The p values are found using the usual chi square
  approximation to the null distribution. If parameters are estimated
  this is done via the method of minimum chi square, see \citet{berkson1980}.
  In all cases bins are combined until all of them have an expected
  count of at least 2. 
	
  The variation of a large number of equal spaced bins using Pearson's formula is abbreviated ES-l-P, and similarly for the other chi-square variants.

  There is a very large literature on chi square tests, the oldest of the
  goodness of fit tests. Discussions of various issues involved in chi square goodness of fit tests can be found in \citet{bogdan1995}, \citet{dahiya1973}, \citet{gontscharuk2016}, \citet{greenwood1996}, \citet{kallenberg1985}, \citet{koehler1990}, \citet{mineo1979}, \citet{ooosterhoff1985}, \citet{quine1985}, \citet{rolke2020}, \citet{voinov2013} and \citet{watson1958}.
	
\end{enumerate}	

\subsection{Discrete (Histogram) Data/Model}


All the methods above are also implemented for discrete data, taking
advantage of the fact that the number of bins is fixed and does not grow
with the sample size. Therefore the discrete versions of the tests run
generally much faster than the continuous ones.

Sometimes data, although in principle continuous, is in fact collected in binned form, so called histogram data. While this is not the same as discrete data it can be turned into that by choosing a point in each bin, for example the midpoints. Moreover, most gof tests do not actually make use of the values of the discrete data, and so the two types can be dealt with in the same way. 

It is worth noting that these discrete versions are based on the
theoretical ideas of the tests and not on the actual formula of
calculation for the continuous case. The test statistics can therefore
be different even when applied to the same data. For example, the
Kolmogorov-Smirnov test is based on the largest absolute difference
between the theoretical (\(F\)) and the empirical (\(\hat{F}\))
distribution functions:

$$D=\max\left\{\vert\hat{F}(x)-F(x) \vert: x\in \mathbb{R} \right\}$$

It can be shown that in the continuous case we have

\[D=\max\left\{F(x_i)-(i-1)/n, i/n-F(x_i):i=1,..,n \right\}\]

where \(x_1,..,x_n\) is the ordered data set. But in the discrete case
the formula becomes

\[D=\max\left\{ \vert F(v_i)-z_i/n\vert:i=1,..,n \right\}\]

where the \(v_i\) are the values of the discrete random variable (or
possibly the midpoints of the the histogram bars) and the \(z_i\) are
the cumulative counts (or heights of the bars).

Now in the continuous case \(\hat{F}\) is a step function but \(F\) is
continuous, and therefore \(D>0\). In the discrete case however
\(\hat{F}\) and \(F\) are both step functions with jumps at the same
points, so if those jumps are all of equal height we have \(D=0\). This
shows that the two cases are fundamentally different.

Another issue is the formula used for calculation. Many software
programs, for example the R routine \(ks.test\), make use of a
relationship between the empirical distribution function and the
quantiles of the data set. However, for discrete data we have many
ties (aka equal observations), and so there is no unique ordering of the
observations and it is not even clear how one should calculate those
quantiles. We avoid those issues by simply using the appropriate
formulas for discrete data.

As in the case of continuous data null distributions are found using
simulation. In fact in the case of discrete data none of the tests has a
known distribution for the test statistic under the null hypothesis, and
so simulation is the only way to go.

For discrete data we also include four variations of chi square tests, again using the formulas by Pearson or
  based on the likelihood ratio test and both a large number and a small
  number of bins. Again the routine combines bins until all have
  expected counts greater than 2, and the chi square approximation is
  used to find p values. Because the binning is already given there is no distinction between equal spaced and equal probability bins.


\section{Simulation Studies}

In order to assess the relative strengths and weakness of the methods we carried out 20 different simulation studies. As an example, consider the following classic case. Under the null hypothesis the data comes from a normal distribution with mean and standard deviation unspecified. In reality the data comes from a \emph{t} distribution with d degrees of freedom. For each run the mean and standard deviation are estimated via the usual sample mean and sample standard deviation. The sample size is 500. d ranges from 1 to 25. For the discrete case the data is then binned into 50 equal sized bins, except for the first and last which are $(-\infty, -2.5)$ and $(2.5, \infty)$. The resulting power graphs are shown in figure~\ref{fig:fig1}. The methods are ordered by their mean power, so we see that for this case Zhang's ZC method has the highest power for both continuous and discrete data. Note that the curves in these graphs have been smoothed, leading to the appearance of a power of over 100\%. 

\begin{figure}
\begin{center}
\includegraphics[width=4in]{fig1.pdf}
\end{center}
\caption{Power curves for Normal vs. t distribution with mean and standard deviation estimated. Sample size is 500. Methods are ordered by mean power and curves are smoothed for easier reading.}
\label{fig:fig1}
\end{figure}

Table~\ref{tab:table1} shows the list of case studies. In all cases the sample size was 500 and the discrete data was based on 50 bins. The first 15 studies are for simple null hypotheses, the last 5 for composite hypotheses. In those studies the parameters were estimated via maximum likelihood.

\begin{table}[H]
\begin{center}
\caption{Case studies included in this article}
\label{tab:table1}
\begin{tabular}{l|c|c|r}
 & Null Hypothesis & True Distribution / Density  & Parameter Range\\
\hline
1 & $U[0,1]$ &  $f(x) = (2sx+1-s)I_{[0,1]}$ & $[0,0.5]$ \\
2 & $U[0,1]$  & $f(x) = \frac{1+a(x-1/2)^2}{1+a/12}I_{[0,1]}$ & $[0,6]$\\
3 & $U[0,1]$  & $(1-\alpha) U[0,1] + \alpha N(0.5, 0.05)$ &  $[0, 0.3]$ \\
4 & $U[0,1]$  & $U[0,1]+$ Spikes & \\
5 & $\text{Beta}(2,2)$ & $\text{Beta}(b,b)$ & $[2,4]$ \\
6 & $\text{Beta}(2,2)$ & $\text{Beta}(2,b)$ & $[2,3]$ \\
7 & $N(0,1)$ & $N(\mu,1)$ & $\mu\in[0,0.4]$  \\
8 & $N(0,1)$ & $N(0,\sigma)$ & $[1,1.4]$  \\
9 & $N(0,1)$ & $t(df)$ & $\{2,4,..,50\}$  \\
10 & $N(0,1)$ & $(1-\alpha)N(0,1)+\alpha N(5,1)$  & $[0, 0.05]$  \\
& & (large positive outliers) & \\
11 &$N(0,1)$ & $(1-\alpha)N(0,1)+\frac{\alpha}{2} N(-5,1)+\frac{\alpha}{2}N(5,1)$ & $[0, 0.05]$  \\
& & (large outliers on both sides) & \\
12 & $Exp(1)$ & Gamma$(1, \beta)$ & $[1,1.3]$  \\
13 & $Exp(1)$ & Weibull$(1, \beta)$ & $[1,1.5]$  \\
14 & $Exp(1)$ & $(1-\alpha) Exp(1) + \alpha N(0.5, 0.05)$ & $[0, 0.15]$ \\
15 & $Exp(1)$ &  Linear & $[-0.5,-1]$ \\  
& trunc. to $[0,1]$ & & \\
\hline
16 & $N(\mu,\sigma)$ & $t(df)$ & $\{1,2,..,25\}$  \\
17 & $Exp(\lambda)$ & Weibull$(1, \beta)$ & $[1,1.5]$  \\
18 & $Exp(\lambda)$ & Linear & $[-0.5,-1]$  \\
& trunc. to $[0,1]$ & & \\
19 & $Exp(\lambda)$ & Gamma$(1, \beta)$ & $[1,1.3]$ \\
20 & $N(\mu,\sigma)$ & Cauchy(s) & $[1,2]$  
\end{tabular}
\end{center}
\end{table}

\section{True Type I Error Probabilities}

The following tables show the true type I error for a nominal type I error of \(\alpha=0.05\).

\textbf{Continuous Data}

\begin{table}[H]
\begin{center}
\caption{True type I error probabilities, continuous data}
\label{tab:table2}
\begin{tabular}{l|c|c|c|c|c|c|c|c|r}
 & KS & Kuiper & AD & CvM &  W & ZA & ZK & ZC & Wassp1\\
\hline
1 & 6.7 &    6.7 & 5.3 & 5.4 & 5.2 & 4.9 & 4.7 & 5.8 & 5.9\\
2 & 4.5 &    4.5 & 4.5 & 4.5 & 4.4 & 5.3 & 5.0 & 4.9 & 5.2\\
3 & 4.4 &    4.4 & 4.6 & 4.5 & 4.3 & 5.2 & 3.3 & 6.5 & 4.7\\
4 & 4.9 &    4.9 & 4.9 & 4.4 & 4.5 & 4.9 & 5.0 & 5.2 & 4.6\\
5 & 4.0 &    4.0 & 3.4 & 3.5 & 4.3 & 5.3 & 4.9 & 5.2 & 3.7\\
6 & 3.3 &    3.3 & 4.1 & 3.4 & 2.9 & 3.6 & 4.1 & 4.1 & 4.0\\
7 & 6.2 &    6.2 & 4.4 & 4.3 & 4.9 & 4.4 & 4.5 & 4.1 & 4.7\\
8 & 4.1 &    4.1 & 4.3 & 4.7 & 5.2 & 5.0 & 6.2 & 6.0 & 4.2\\
10 & 5.6 &    5.6 & 4.7 & 4.1 & 5.0 & 5.2 & 4.5 & 4.3 & 5.1\\
11 & 5.5 &    5.5 & 5.1 & 5.8 & 6.4 & 5.6 & 4.4 & 5.4 & 4.2\\
12 & 5.0 &    5.0 & 4.7 & 4.6 & 5.3 & 6.8 & 4.4 & 6.5 & 5.6\\
13 & 5.3 &    5.3 & 5.7 & 5.7 & 5.6 & 4.7 & 6.1 & 6.0 & 5.2\\
14 & 6.9 &    6.9 & 7.6 & 7.5 & 5.4 & 5.8 & 5.1 & 4.8 & 6.8 \\
17 & 5.2 &    5.2 & 4.8 & 5.2 & 5.1 & 4.8 & 5.2 & 4.4 & 5.7
\end{tabular}
\end{center}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{l|c|c|c|c|c|c|c|r}
 & EP-l-P & ES-l-P & EP-s-P & ES-s-P & EP-l-LR & ES-l-LR & EP-s-LR & ES-s-LR \\
\hline
1 & 2.4 & 5.5 & 5.0 & 5.2 & 2.1 & 6.1 & 4.2 & 4.9 \\
2 & 3.2 & 6.6 & 6.8 & 6.3 & 2.4 & 7.1 & 5.8 & 5.9 \\
3 & 3.2 & 5.1 & 5.0 & 4.8 & 2.4 & 5.3 & 4.3 & 4.8 \\
4 & 2.6 & 4.6 & 5.2 & 5.4 & 1.5 & 5.5 & 4.8 & 5.2 \\
5 & 2.4 & 3.5 & 6.0 & 4.8 & 2.2 & 4.4 & 5.3 & 4.7 \\
6 & 3.1 & 4.3 & 5.9 & 4.9 & 2.5 & 4.3 & 4.8 & 4.9 \\
7 & 2.3 & 3.9 & 6.0 & 5.5 & 1.9 & 4.5 & 4.8 & 5.4 \\
8 & 2.9 & 4.4 & 4.7 & 5.5 & 1.9 & 4.4 & 3.7 & 5.9 \\
10 & 3.5 & 4.5 & 6.4 & 6.2 & 2.6 & 5.5 & 5.6 & 5.3 \\
11 & 2.0 & 4.2 & 6.0 & 5.5 & 1.7 & 4.7 & 4.9 & 4.5 \\
12 & 2.2 & 5.1 & 4.9 & 5.7 & 1.5 & 5.8 & 4.5 & 5.1 \\
13 & 2.7 & 5.4 & 4.6 & 5.4 & 1.9 & 5.6 & 4.2 & 5.0 \\
14 & 2.5 & 6.1 & 6.6 & 6.2 & 1.9 & 6.9 & 5.8 & 5.6 \\
17 & 3.8 & 4.2 & 5.5 & 4.1 & 2.1 & 5.3 & 5.1 & 5.2 \\
\end{tabular}
\end{center}
\end{table}



\textbf{Discrete Data}

\begin{table}[H]
\begin{center}
\caption{True type I error probabilities, discrete data}
\label{tab:table3}
\begin{tabular}{l|c|c|c|c|c|c|c|r}
 & KS & K & AD & CvM & W & ZA & ZC & Wassp1\\
\hline
1 & 5.4 & 4.8 & 6.1 & 6.3 & 4.6 & 6.6 & 5.4 & 6.2 \\
2 & 4.4 & 4.6 & 4.0 & 4.1 & 4.5 & 3.7 & 5.5 & 4.8 \\
3 & 4.8 & 4.1 & 4.4 & 4.1 & 4.1 & 4.2 & 4.9 & 4.5 \\
4 & 5.4 & 5.2 & 4.7 & 5.3 & 4.9 & 4.1 & 4.2 & 4.9 \\
5 & 4.7 & 3.9 & 4.5 & 4.4 & 3.4 & 3.7 & 4.0 & 4.0 \\
6 & 5.3 & 5.3 & 5.6 & 6.2 & 5.7 & 4.2 & 4.8 & 5.5 \\
7 & 6.2 & 6.0 & 5.4 & 5.4 & 6.5 & 4.5 & 5.2 & 5.4 \\
8 & 5.2 & 5.5 & 5.4 & 5.9 & 6.9 & 5.5 & 4.5 & 5.5 \\
10 & 4.8 & 4.1 & 6.2 & 5.1 & 3.6 & 5.6 & 5.6 & 6.4 \\
11 & 4.7 & 6.1 & 5.7 & 5.0 & 4.8 & 5.0 & 4.6 & 5.5 \\
12 & 6.0 & 4.3 & 5.9 & 6.4 & 5.8 & 5.9 & 6.3 & 5.9 \\
13 & 4.6 & 5.5 & 5.3 & 5.3 & 4.5 & 5.0 & 5.5 & 6.0 \\
14 & 5.1 & 4.9 & 4.2 & 4.3 & 4.0 & 6.2 & 6.1 & 5.1 \\
17 & 5.1 & 5.0 & 6.1 & 5.8 & 3.7 & 0.3 & 5.5 & 6.0 \\
\end{tabular}
\end{center}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{l|c|c|c|r}
 & l-P & s-P & l-LR & s-LR \\
\hline
1 & 4.9 & 3.0 & 5.9 & 3.1 \\
2 & 5.1 & 3.3 & 5.7 & 3.0 \\
3 & 5.0 & 2.8 & 5.5 & 2.9 \\
4 & 4.1 & 3.2 & 5.5 & 3.2 \\
5 & 4.4 & 2.8 & 5.2 & 3.3 \\
6 & 3.6 & 2.9 & 4.1 & 3.3 \\
7 & 3.7 & 2.7 & 4.4 & 3.0 \\
8 & 4.3 & 2.9 & 4.7 & 2.7 \\
10 & 4.0 & 3.7 & 5.1 & 3.8 \\
11 & 3.4 & 3.0 & 3.4 & 3.2 \\
12 & 4.8 & 3.4 & 4.8 & 3.2 \\
13 & 4.0 & 3.7 & 4.1 & 3.4 \\
14 & 5.0 & 3.6 & 5.1 & 3.6 \\
17 & 2.7 & 1.5 & 1.6 & 1.6 \\
\end{tabular}
\end{center}
\end{table}


As we can see, all the methods achieve the correct type I error rate of
\(5\%\), within simulation error. In the discrete case many have an
actual type I error much smaller than \(5\%\), as is often the case for
discrete data.

Note that in Case 9: Normal - t, Case 16: Normal - t, estimated, Case
15: Truncated Exponential - Linear and Case 18: Truncated Exponential -
Linear, estimated the null hypothesis is always wrong, and therefore no
check of the type I error is possible.

\section{Power}

The following table shows the powers of all the methods at the smallest
value of the parameter where at least one method has a power higher then
\(80\%\):

\textbf{Continuous Data}

\begin{table}[H]
\begin{center}
\caption{Power of all methods when highest power is just above 80\%, continuous data}
\label{tab:table4}
\begin{tabular}{l|c|c|c|c|c|c|c|c|r}
 & KS & Kuiper & AD & CvM &  W & ZA & ZK & ZC & Wassp1\\
\hline
1 & 81.6 & 81.6 & 83.8 & 83.2 & 57.3 & 65.3 & 66.8 & 65.6 & 84.6 \\
2 & 45.0 & 45.0 & 64.8 & 46.9 & 85.4 & 42.4 & 57.8 & 48.7 & 47.3 \\
3 & 39.8 & 40.1 & 33.6 & 36.2 & 84.0 & 15.4 & 30.1 & 20.9 & 20.0 \\
4 & 22.9 & 22.9 & 41.5 & 25.9 & 9.5 & 54.0 & 53.4 & 50.1 & 28.1 \\
5 & 24.2 & 24.2 & 50.2 & 28.7 & 80.5 & 89.3 & 65.9 & 86.4 & 47.1 \\
6 & 70.1 & 70.1 & 80.4 & 77.2 & 45.2 & 72.4 & 69.5 & 72.2 & 81.7 \\
7 & 75.3 & 75.3 & 80.2 & 78.6 & 43.9 & 66.4 & 63.6 & 68.1 & 81.5 \\
8 & 27.8 & 27.8 & 68.6 & 33.8 & 78.2 & 82.9 & 82.9 & 88.6 & 77.7 \\
9 & 5.3 & 5.3 & 16.3 & 7.4 & 13.0 & 74.4 & 64.7 & 85.7 & 26.7 \\
10 & 14.9 & 14.9 & 32.9 & 15.5 & 13.2 & 70.5 & 90.7 & 78.4 & 36.7 \\
11 & 8.1 & 8.1 & 32.7 & 10.5 & 29.1 & 78.7 & 86.2 & 88.8 & 42.9 \\
12 & 69.9 & 69.9 & 78.7 & 74.9 & 47.2 & 78.0 & 70.8 & 76.7 & 85.0 \\
13 & 68.4 & 68.4 & 79.8 & 74.9 & 40.9 & 71.0 & 71.9 & 75.1 & 88.8 \\
14 & 48.6 & 48.6 & 32.7 & 37.4 & 76.5 & 20.5 & 23.5 & 18.1 & 20.5 \\
15 & 77.5 & 77.5 & 84.8 & 78.5 & 58.8 & 82.1 & 80.7 & 80.8 & 85.0 \\
16 & 60.0 & 60.0 & 80.6 & 75.0 & 76.2 & 84.2 & 78.8 & 87.2 & 84.0 \\
17 & 76.3 & 76.3 & 87.2 & 85.3 & 72.5 & 81.7 & 70.0 & 80.3 & 75.6 \\
18 & 62.3 & 62.3 & 81.2 & 76.7 & 74.9 & 66.2 & 58.5 & 63.1 & 83.5 \\
19 & 67.0 & 67.0 & 82.4 & 76.8 & 65.8 & 81.4 & 72.2 & 79.1 & 54.5 \\
20 & 22.1 & 22.1 & 8.3 & 11.2 & 33.8 & 2.2 & 7.2 & 3.4 & 1.7 \\
\end{tabular}
\end{center}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{l|c|c|c|c|c|c|c|r}
 & EP-l-P & ES-l-P & EP-s-P & ES-s-P & EP-l-LR & ES-l-LR & EP-s-LR & ES-s-LR \\
\hline
1 & 13.3 & 26.9 & 47.1 & 47.1 & 9.9 & 29.2 & 45.1 & 48.1 \\
2 & 21.4 & 41.6 & 65.7 & 62.6 & 16.7 & 39.4 & 63.3 & 62.3 \\
3 & 40.4 & 39.7 & 86.6 & 82.3 & 30.5 & 31.5 & 83.5 & 79.1 \\
4 & 65.9 & 79.0 & 74.6 & 79.1 & 63.1 & 80.7 & 72.5 & 80.0 \\
5 & 22.6 & 30.5 & 55.2 & 60.5 & 23.9 & 26.0 & 55.9 & 67.8 \\
6 & 19.7 & 18.3 & 53.4 & 51.4 & 18.0 & 21.8 & 53.6 & 53.9 \\
7 & 11.8 & 23.8 & 45.4 & 49.5 & 9.3 & 22.4 & 43.6 & 47.4 \\
8 & 38.8 & 45.4 & 69.8 & 85.5 & 30.4 & 42.2 & 65.8 & 80.2 \\
9 & 12.7 & 13.8 & 16.0 & 41.3 & 7.8 & 14.5 & 14.0 & 33.3 \\
10 & 30.1 & 46.6 & 28.4 & 71.1 & 16.2 & 33.6 & 24.5 & 61.7 \\
11 & 50.5 & 33.1 & 29.7 & 82.1 & 33.0 & 27.3 & 26.7 & 74.8 \\
12 & 12.6 & 11.6 & 41.6 & 35.4 & 10.9 & 18.1 & 41.8 & 45.6 \\
13 & 16.9 & 42.7 & 50.5 & 71.4 & 12.2 & 33.4 & 47.3 & 64.3 \\
14 & 52.1 & 69.2 & 86.9 & 15.0 & 39.0 & 63.6 & 83.8 & 17.0 \\
15 & 21.7 & 21.6 & 60.7 & 59.4 & 22.0 & 23.6 & 62.0 & 64.1 \\
16 & 9.8 & 11.8 & 10.6 & 33.2 & 6.8 & 14.2 & 10.6 & 33.0 \\
17 & 13.0 & 22.0 & 37.6 & 26.5 & 10.3 & 21.3 & 36.3 & 29.9 \\
18 & 10.7 & 16.0 & 45.1 & 51.3 & 6.8 & 17.3 & 44.9 & 52.7 \\
19 & 10.3 & 18.9 & 32.5 & 15.7 & 7.9 & 19.3 & 32.5 & 19.1 \\
20 & 40.3 & 32.2 & 57.4 & 85.1 & 35.7 & 33.7 & 56.7 & 84.4 \\
\end{tabular}
\end{center}
\end{table}



\textbf{Discrete Data}

\begin{table}[H]
\begin{center}
\caption{Power of all methods when highest power is just above 80\%, discrete data}
\label{tab:table5}
\begin{tabular}{l|c|c|c|c|c|c|c|r}
 & KS & K & AD & CvM & W & ZA & ZC & Wassp1\\
\hline
1 & 75.4 & 56.4 & 83.2 & 83.0 & 51.7 & 79.0 & 61.3 & 83.5 \\
2 & 38.5 & 77.9 & 55.8 & 35.4 & 83.8 & 35.2 & 37.6 & 37.8 \\
3 & 57.1 & 88.2 & 38.7 & 35.9 & 87.1 & 7.1 & 3.1 & 30.5 \\
4 & 32.3 & 29.6 & 48.8 & 34.3 & 13.5 & 85.9 & 73.5 & 36.2 \\
5 & 41.2 & 85.4 & 74.2 & 48.9 & 88.1 & 81.6 & 63.6 & 62.1 \\
6 & 76.9 & 59.9 & 83.5 & 81.7 & 33.2 & 13.0 & 8.6 & 82.3 \\
7 & 78.1 & 48.9 & 84.6 & 82.3 & 62.7 & 89.6 & 85.1 & 86.1 \\
8 & 43.5 & 85.8 & 79.9 & 46.4 & 85.1 & 85.3 & 87.0 & 86.3 \\
9 & 8.7 & 26.2 & 32.0 & 7.0 & 20.7 & 74.1 & 85.4 & 45.8 \\
10 & 13.4 & 14.9 & 47.4 & 18.3 & 8.7 & 73.8 & 87.2 & 48.6 \\
11 & 9.0 & 52.1 & 63.5 & 13.0 & 32.3 & 75.9 & 89.5 & 71.9 \\
12 & 67.5 & 42.0 & 76.2 & 72.8 & 61.9 & 4.5 & 0.2 & 81.9 \\
13 & 68.2 & 48.0 & 78.2 & 73.5 & 7.5 & 84.3 & 82.4 & 86.7 \\
14 & 51.4 & 76.5 & 35.8 & 40.0 & 80.5 & 8.9 & 4.2 & 28.5 \\
15 & 80.2 & 62.5 & 87.2 & 82.7 & 73.7 & 3.6 & 0.1 & 87.1 \\
16 & 11.1 & 15.5 & 43.2 & 17.2 & 15.6 & 81.2 & 79.6 & 37.9 \\
17 & 80.8 & 73.9 & 88.9 & 87.8 & 85.8 & 28.7 & 20.5 & 77.4 \\
18 & 62.2 & 70.7 & 78.5 & 74.4 & 80.1 & 39.4 & 16.2 & 81.0 \\
19 & 66.4 & 53.3 & 85.0 & 80.7 & 73.8 & 10.3 & 5.9 & 48.8 \\
20 & 32.1 & 80.5 & 12.9 & 15.6 & 74.6 & 13.5 & 19.8 & 8.2 \\
\end{tabular}
\end{center}
\end{table}

\begin{table}[H]
\begin{center}
\begin{tabular}{l|c|c|c|r}
 & l-P & s-P & l-LR & s-LR \\
\hline
1 & 17.9 & 40.8 & 20.3 & 42.9 \\
2 & 27.8 & 59.8 & 31.3 & 58.0 \\
3 & 60.5 & 78.5 & 56.1 & 75.9 \\
4 & 90.3 & 90.0 & 92.0 & 90.6 \\
5 & 38.0 & 71.3 & 47.0 & 74.5 \\
6 & 20.8 & 43.6 & 24.3 & 45.2 \\
7 & 23.1 & 42.9 & 24.8 & 43.4 \\
8 & 67.9 & 65.5 & 59.3 & 64.6 \\
9 & 42.5 & 11.7 & 35.6 & 12.7 \\
10 & 36.7 & 6.4 & 29.6 & 7.7 \\
11 & 73.7 & 9.2 & 63.3 & 10.7 \\
12 & 15.6 & 34.1 & 23.7 & 37.2 \\
13 & 34.9 & 45.8 & 25.9 & 43.8 \\
14 & 73.8 & 30.0 & 70.2 & 32.8 \\
15 & 25.1 & 59.7 & 35.4 & 64.8 \\
16 & 22.8 & 11.1 & 29.4 & 11.9 \\
17 & 24.0 & 45.6 & 18.8 & 50.4 \\
18 & 16.0 & 42.4 & 18.1 & 43.8 \\
19 & 22.2 & 30.7 & 21.8 & 34.7 \\
20 & 46.5 & 43.7 & 51.7 & 45.2 \\
\end{tabular}
\end{center}
\end{table}


In all cases the powers differ widely, with no clear pattern of which
methods are best. Any one method can perform very well in one case and
very poorly in another.

In case 20: Normal vs t with estimated mean and standard deviation the power is always quite small.

\section{Relative Powers}

 To get a
clearer picture of the relative performance we proceed as follows. For
each case a method is assigned a 1 if its power it at least \(90\%\) of
the power of the best method, and a 0 otherwise. Then we count how often
a method is close to best.

\begin{table}[H]
\centering\begingroup\fontsize{14}{17}\selectfont
\label{tab:table6}
\caption{Number of times a method has power close to best}
\begin{tabular}{ll||ll}
\hline
\multicolumn{2}{c}{Continuous Data} & \multicolumn{2}{c}{Discrete Data}\\
\hline
Method & \# Best & Method & \# Best\\
\hline
ZC & 9 & AD & 10\\
AD & 9 & Wassp1 & 8\\
Wassp1 & 8 & W & 8\\
ZA & 7 & ZC & 7\\
CvM & 7 & ZA & 7\\
ES-s-P & 5 & CvM & 7\\
ZK & 5 & K & 6\\
ES-s-LR & 4 & KS & 4\\
EP-s-P & 3 & l-P & 2\\
W & 3 & s-LR& 1\\
Kuiper & 3 & l-LR & 1\\
KS & 3 & s-P & 1\\
EP-s-L & 2 &  & \\
ES-l-L & 1 &  & \\
ES-l-P & 1 &  & \\
EP-l-L & 0 &  & \\
EP-l-P & 0 &  & \\
\hline
\end{tabular}
\endgroup{}
\end{table}

\section{Best Default Selection}

One can of course simply run all the methods on the two data sets. This,
however will lead to a severe simultaneous testing problem. Therefore we
want to find as small a selection of methods that nevertheless includes
the best method (within simulation error) in each of the case studies
included.

To do so we find all combinations of k methods to see whether they
include at least one method that is within \(90\%\) of the best. If so
it also finds the mean power of this selection over all cases.

\textbf{Continuous Data}

It turns out one needs to include six methods. W, ZK, ZC and Wassp1 appear
in all of them. In addition one of the four chi square versions with a
small number of bins is useful, either with equal size or equal
probability bins. The one with the highest mean power is EP-s-P, ES-s-P, W, Wassp1, ZK and ZC, which therefore is our
default selection.
\textbf{Discrete Data}

Here we only need four methods. There are ten selections of four methods that would work. AD is the only method in all
these selections, ZC appears in 8 of the 10. The one with the highest
mean power is Kuiper, AD, ZA and ZC, which therefore is our default
selection. Note that there is no chi square test in this selection.

\section{Conclusions}

We presented the results of simulations studying the power of a number of goodness-of-fit tests for univariate data. The study includes both continuous and discrete data and all of the most commonly used methods. The \proglang{R} package \emph{Rgof} has routines that allow the user to run these tests as well as carry out their own simulation studies. The package also allows the user to add other methods to the list.

Based on these studies we recommend the use of Wilson's test, the ZK and ZC variants of Zhang's tests, a test based on the Wasserstein p=1 measure as well as chi square tests with a small number of equal spaced and equal probability bins as a default choice for continuous data. Note that neither the Kolmogorov-Smirnov test nor the Anderson-Darling test, the two most commonly used ones today, make this short list.

For discrete data we recommend Kuiper's test, Anderson-Darling and the ZA and ZC variants of Zhang's tests.


\bibliographystyle{Chicago}

\bibliography{references}
\end{document}
